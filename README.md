# DPIR Introduction to LLMs Course

## Welcome! 
This is a repository with the collection of teaching materials for the (Large) Language Models for Social Sciences course, thought at the DPIR Oxford in Spring 2025.

The course introduces LLMs from a social science perspective, focusing on how they work and applying them effectively to real-
world research tasks. To that end, the course provides both intuitions into key concepts and techniques behind current LLM-based systems (chat-bots, agents), and provides hands-on coding examples and exercises in Python, to help you strengthen your understanding and develop practical skills. 

## LLM Fundamentals 

This is a key session in which you will learn what language models are, how they are build, and how they generate text sequences. When you are intexracting with, say, a chat-bot, there are multiple fascinating concepts and ideas at play (what is a meaning of a word? what is a meaning of a sentence? how do we encode them?), as well as smart and elegant (and at times redundant) algorithms (tokenization, attention, backpropagation, etc.) that put these concepts to life.

Their particular implementations determine how well a model perfoms, both as a general model of language and as a tool for solving your task. For instance, choosing appropriate way to tokenize text can make or break model's ability to write code. Therefore, having a good understanding of each of the building blocks of an LLM is of great importance. 

This session introduces: 
- What does it mean to build a model of a language? 
- What do we practically need to do to build a language model?
- How to process human readable text input into machine readable input and back?
- Model architecture (transformer and attention)
- How does a language model generate text 

All of these concepts are introduced in the notebook here. To create it, I relied on a selection of an amazing set of educational ressources, which I list below and highly reccomend to you that you read and interact with them: 

### Core ressources 

- [Language Modelling NLP Course for You](https://lena-voita.github.io/nlp_course/language_modeling.html).
-[Hugging Face NLP Course (Chapter 6): Byte-Pair Encoding](https://huggingface.co/learn/nlp-course/en/chapter6/5).
- [Explore Tokenizers via Tiktokenizer app](https://tiktokenizer.vercel.app/).


### Extra ressources 
- [Perplexity of fixed-length models
by ðŸ¤—](https://huggingface.co/docs/transformers/en/perplexity).
- [Andrej Karpathy's "Let's Build a GPT Tokenizer" video](https://www.youtube.com/watch?v=zduSFxRajkE).

### Papers 

- ["Attention is all you need" by Vaswani et al., 2017](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
- ["Neural Machine Translation of Rare Words with Subword Units"
by Sennrich et al. (2015)](https://arxiv.org/abs/1508.07909)

## 2025



## DPIR Methods Spring School Materials 


