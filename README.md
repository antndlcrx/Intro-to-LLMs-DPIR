# DPIR Spring School 2025

## üß† What We‚Äôve Covered So Far

### ‚ú® What Is a Language Model?  
A **language model (LM)** estimates how likely a sequence of words or tokens is.  
It does this by breaking the sequence into **conditional probabilities**:

$$
P(w_1, w_2, ..., w_n) = P(w_1) \cdot P(w_2|w_1) \cdot P(w_3|w_1, w_2) \cdot ... \cdot P(w_n | w_1, ..., w_{n-1})
$$

‚û°Ô∏è *Why it matters:* If a model can assign high probability to real, grammatical, meaningful text ‚Äî it likely understands something about how language works.

---

### ‚úÇÔ∏è Tokenization  
Before training or inference, text is **tokenized**:  
- Split into chunks (tokens), which could be words, subwords, or characters.  
- Each token is mapped to a unique ID for the model to process.

$$
\text{Text} \rightarrow \text{Tokens} \rightarrow \text{Token IDs} \rightarrow \text{Embeddings}
$$

‚û°Ô∏è *Why it matters:* Tokenization defines the granularity of information the model sees, and can introduce bias or loss of meaning (especially across languages/dialects).

---

### üß± Word Embeddings  
Words are represented as **vectors** in a continuous space, learned from co-occurrence patterns in text. These **embeddings** allow models to compute relationships between words (e.g., similarity, analogy).  
‚û°Ô∏è *Why it matters:* Word embeddings are the **first layer** in a language model ‚Äî every token gets converted into an embedding before any processing begins.

---

### üîß Training an LM  
To learn these probabilities, we train a neural network to predict the **next token** in a sequence, given its context.  
This is done via **gradient descent**:  
- The model outputs a probability distribution over the vocabulary.  
- We compute **cross-entropy loss** between predicted and actual tokens.  
- Gradients of this loss are used to update parameters.  
‚û°Ô∏è *Why it matters:* Training teaches the model to encode linguistic patterns, world knowledge, and context dependencies.

---

### üß† Architecture: The Neural Network  
At the core of every LM is a neural architecture that turns token embeddings into predictions.  
We started with simple architectures like **RNNs**, which process tokens one at a time. But RNNs struggle with long-range dependencies and parallelization.

‚û°Ô∏è *Why it matters:* The **architecture defines the model's capacity** to learn from context. That‚Äôs why Transformers (next!) were such a breakthrough.

---

### üîÄ The Transformer  
Transformers process entire sequences **at once**, using **self-attention** to decide which tokens to pay attention to.  
They consist of repeated blocks with:
- **Multi-head attention**: lets each token learn from all others.
- **Feedforward layers**: refine the token's internal representation.
- **LayerNorm, skip connections, and dropout**: help stabilize and generalize learning.

‚û°Ô∏è *Why it matters:* Nearly all modern LLMs (GPT, BERT, LLaMA) are built on Transformers. It‚Äôs the most important architectural advance in NLP.

---

### ü§ó Hugging Face Ecosystem  
We introduced the **Hugging Face Hub**, a platform for sharing and using models, datasets, and evaluation tools. Throughout the course, we will work with several of Hugging Face‚Äôs most powerful libraries:

- [`transformers`](https://github.com/huggingface/transformers): For accessing and running pretrained models like GPT-2, BERT, and more.  
- [`datasets`](https://huggingface.co/docs/datasets): For loading, manipulating, and analyzing NLP datasets at scale.  
- [`pipeline`](https://huggingface.co/docs/transformers/main_classes/pipelines): A high-level interface for common tasks like generation, classification, and translation.

These tools allow us to:
- Quickly prototype LLM-based tasks  
- Explore and evaluate bias  
- Analyze model predictions with minimal code

‚û°Ô∏è *Why it matters:* The Hugging Face ecosystem has become the de facto standard for working with LLMs in both research and industry. Mastering it will give you the ability to build powerful workflows, test your ideas, and run real-world analyses.

--- 

### ‚öñÔ∏è Bias in Language Models  
We discussed how LMs trained on web-scale data can **amplify social bias**:
- From training corpora (representation gaps, slurs)
- From tokenization (some dialects get fragmented)
- From alignment processes (e.g., fine-tuning with biased human feedback)

We introduced **HolisticBias** as a benchmark to evaluate completions across social identities, and used the `pipeline` API to analyze generated text for sentiment.

‚û°Ô∏è *Why it matters:* Social scientists must understand and **audit model behavior**, especially when applying LMs to people-centered domains.

---

## üî• Today‚Äôs Focus: Transformers & Pretrained Models

---

### üß© Prompt Engineering  
Learn how to guide model behavior with well-crafted prompts ‚Äî especially for tasks like classification, sentiment and stance prediction, and using LMs as proxies for studying human behaviour.

### üß™ Fine-Tuning  
We‚Äôll explore how to further **train** a model on your own dataset to better match your domain or goals.

### üß† Preference Tuning (Instruction Tuning + RLHF)
Learn how models are aligned with human values and goals.
We‚Äôll cover:

Instruction Tuning ‚Äì training models to follow natural language instructions

RLHF ‚Äì a method for teaching models to prefer responses humans rate as more helpful, safe, or aligned
These methods are essential for developing assistant-like LMs and understanding how subjective judgments get encoded into model behavior.

### üìä Social Science Applications  
Use models to:
- Simulate or emulate human responses
- Analyze public opinion

---

## üëÄ Coming Up Next

Tomorrow, we shift focus to **applying LLMs to tasks**:


### üß† Reasoning and Alignment  
We‚Äôll explore:
- When models can reason, and why
- What is reasoning to begin with and where can it be helpful? 
- How to endow models with tools

---


# DPIR Spring School 2024
This repository contains material for the workshops taught at the DPIR Spring School 2024

- [Learning Python](https://github.com/antndlcrx/oss_2024/blob/main/tutorials/oss_python_intro.ipynb)
- [Introduction to LLMs](https://github.com/antndlcrx/oss_2024/blob/main/tutorials/oss_python_intro.ipynb)


